{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "ba6c7e19",
      "metadata": {
        "id": "ba6c7e19"
      },
      "source": [
        "# Introduction to pyvene\n",
        "This tutorial shows simple runnable code snippets of how to do different kinds of interventions on neural networks with pyvene.\n",
        "\n",
        "This is a simplified version of the original notebook, that only introduces key concepts."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0706e21b",
      "metadata": {
        "id": "0706e21b"
      },
      "source": [
        "## Set-up"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "a6B4nY4q43Pv"
      },
      "id": "a6B4nY4q43Pv",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e08304ea",
      "metadata": {
        "id": "e08304ea"
      },
      "outputs": [],
      "source": [
        "try:\n",
        "    # This library is our indicator that the required installs\n",
        "    # need to be done.\n",
        "    import pyvene as pv\n",
        "\n",
        "except ModuleNotFoundError:\n",
        "    !pip install git+https://github.com/stanfordnlp/pyvene.git\n",
        "    import pyvene as pv"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0ede4f94",
      "metadata": {
        "id": "0ede4f94"
      },
      "source": [
        "## pyvene 101\n",
        "Before we get started, here are a couple of core notations that are used in this library:\n",
        "- **Base** example: this is the example we are intervening on, or, we are intervening on the computation graph of the model running the **Base** example.\n",
        "- **Source** example or representations: this is the source of our intervention. We use **Source** to intervene on **Base**.\n",
        "- **component**: this is the `nn.module` we are intervening in a pytorch-based NN. For models supported by this library, you can use directly access via str, or use the abstract names defined in the config file (e.g., `h[0].mlp.output` or `mlp_output` with other fields).\n",
        "- **unit**: this is the axis of our intervention. If we say our **unit** is `pos` (`position`), then you are intervening on each token position.\n",
        "- **unit_locations**: this list gives you the percisely location of your intervention. It is the locations of the unit of analysis you are specifying. For instance, if your `unit` is `pos`, and your `unit_location` is 3, then it means you are intervening on the third token. If this field is left as `None`, then no selection will be taken, i.e., you can think of you are getting the raw tensor and you can do whatever you want.\n",
        "- **intervention_type** or **intervention**: this field specifies the intervention you can perform. It can be a primitive type, or it can be a function or a lambda expression for simple interventions. One benefit of using primitives is speed and systematic training schemes. You can also save and load interventions if you use the supported primitives."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7245643b-fd44-47a5-a189-ce1565da7e25",
      "metadata": {
        "id": "7245643b-fd44-47a5-a189-ce1565da7e25"
      },
      "source": [
        "### Workflow: Wrap and Intervene\n",
        "The usual workflow for using pyvene is to load a model, define an intervention config and wrap the model, and then run the intervened model. This returns both the original and intervened outputs, as well as any internal activations you specified to collect.\n",
        "\n",
        "For example: Setting activations to zero"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "17c7f2f6-b0d3-4fe2-8e4f-c044b93f3ef0",
      "metadata": {
        "id": "17c7f2f6-b0d3-4fe2-8e4f-c044b93f3ef0",
        "outputId": "7def9679-8749-43ed-8743-fe315d743af1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "nnsight is not detected. Please install via 'pip install nnsight' for nnsight backend.\n",
            "Original Output: \n",
            " of the, Madrid\n",
            "Intervened Output: \n",
            " of the, the\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "import pyvene as pv\n",
        "\n",
        "# 1. Load the model\n",
        "model_name = \"gpt2\"\n",
        "gpt2 = AutoModelForCausalLM.from_pretrained(model_name, attn_implementation=\"eager\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "\n",
        "\n",
        "# 2. Wrap the model\n",
        "pv_gpt2 = pv.IntervenableModel({\n",
        "    \"layer\": 0,                                                         # Layer to intervene on\n",
        "    \"component\": \"mlp_output\",                                          # Component to intervene on\n",
        "    \"source_representation\": torch.zeros(gpt2.config.n_embd)            # Intervention to be performed\n",
        "}, model=gpt2)\n",
        "\n",
        "\n",
        "# 3. Run the intervened model\n",
        "orig_outputs, intervened_outputs = pv_gpt2(\n",
        "    base = tokenizer(\"The capital of Spain is\", return_tensors=\"pt\"),     # Input to intervene on\n",
        "    unit_locations={\"base\": 3},                                           # Input tokens to intervene on\n",
        "    output_original_output=True # False then the first element in the tuple is None\n",
        ")\n",
        "\n",
        "\n",
        "# 4. Compare outputs\n",
        "# print(intervened_outputs.logits)\n",
        "# print(orig_outputs.logits)\n",
        "\n",
        "# Look at the prediction of the clean run versus the intervened run:\n",
        "# Get logits\n",
        "orig_logits = orig_outputs.logits\n",
        "intervened_logits = intervened_outputs.logits\n",
        "\n",
        "# Convert logits to token predictions\n",
        "orig_predictions = orig_logits.argmax(dim=-1)  # Select most likely token at each position\n",
        "intervened_predictions = intervened_logits.argmax(dim=-1)\n",
        "\n",
        "# Decode token predictions to text\n",
        "orig_text = tokenizer.decode(orig_predictions[0])\n",
        "intervened_text = tokenizer.decode(intervened_predictions[0])\n",
        "\n",
        "print(\"Original Output:\", orig_text)\n",
        "print(\"Intervened Output:\", intervened_text)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1410904d",
      "metadata": {
        "id": "1410904d"
      },
      "source": [
        "### Interchange Interventions\n",
        "Instead of a static vector (e.g., zero), we can intervene the model with activations sampled from a different forward run. We call this interchange intervention, where intervention happens between two examples and we are interchanging activations between them."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9691c7d8",
      "metadata": {
        "id": "9691c7d8",
        "outputId": "caf7fde0-1279-414c-b02b-ece4a5a5ab49",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loaded model\n",
            "Original Output: \n",
            " of the, Rome \n",
            "Intervened Output: \n",
            " of the, the \n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import pyvene as pv\n",
        "\n",
        "# 1. Load the model\n",
        "# built-in helper to get a HuggingFace model - we use gpt2 with an LM head here\n",
        "_, tokenizer, gpt2 = pv.create_gpt2_lm()\n",
        "\n",
        "# Define a config\n",
        "pv_config = pv.IntervenableConfig([{\n",
        "  \"layer\": 0,\n",
        "  \"component\": \"mlp_output\"},\n",
        "  {\n",
        "  \"layer\": 1,\n",
        "  \"component\": \"mlp_output\"}],\n",
        "  intervention_types=pv.VanillaIntervention\n",
        ")\n",
        "\n",
        "# 2. Wrap the model\n",
        "pv_gpt2 = pv.IntervenableModel(\n",
        "  pv_config, model=gpt2)\n",
        "\n",
        "\n",
        "# 3. Run the intervened model\n",
        "orig_outputs, intervened_outputs = pv_gpt2(\n",
        "  base=tokenizer(\"The capital of Italy is \",return_tensors = \"pt\"),      # Base, i.e., intervened on\n",
        "  sources=tokenizer(\"The capital of Spain is \", return_tensors = \"pt\"),  # Source, i.e, intervened with\n",
        "  unit_locations={\"sources->base\": 3},\n",
        "  output_original_output=True\n",
        ")\n",
        "\n",
        "# Look at the prediction of the clean run versus the intervened run:\n",
        "# Get logits\n",
        "orig_logits = orig_outputs.logits\n",
        "intervened_logits = intervened_outputs.logits\n",
        "\n",
        "# Convert logits to token predictions\n",
        "orig_predictions = orig_logits.argmax(dim=-1)  # Select most likely token at each position\n",
        "intervened_predictions = intervened_logits.argmax(dim=-1)\n",
        "\n",
        "# Decode token predictions to text\n",
        "orig_text = tokenizer.decode(orig_predictions[0])\n",
        "intervened_text = tokenizer.decode(intervened_predictions[0])\n",
        "\n",
        "print(\"Original Output:\", orig_text)\n",
        "print(\"Intervened Output:\", intervened_text)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9c5b2270",
      "metadata": {
        "id": "9c5b2270"
      },
      "source": [
        "### Addition Intervention\n",
        "Activation swap is one kind of interventions we can perform. Here is another simple one: `pv.AdditionIntervention`, which adds the sampled representation into the **Base** run."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a40f5989",
      "metadata": {
        "id": "a40f5989",
        "outputId": "e3dce831-c9ea-4ab1-fee2-f45cbb93daec"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "loaded model\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import pyvene as pv\n",
        "\n",
        "# 1. Load model\n",
        "_, tokenizer, gpt2 = pv.create_gpt2()\n",
        "\n",
        "# 2. Wrap model\n",
        "config = pv.IntervenableConfig({\n",
        "    \"layer\": 0,\n",
        "    \"component\": \"mlp_input\"},\n",
        "    pv.AdditionIntervention\n",
        ")\n",
        "\n",
        "pv_gpt2 = pv.IntervenableModel(config, model=gpt2)\n",
        "\n",
        "# 3. Run on intervened model\n",
        "intervened_outputs = pv_gpt2(\n",
        "    base = tokenizer(\n",
        "        \"The Space Needle is in downtown\",\n",
        "        return_tensors=\"pt\"\n",
        "    ),\n",
        "    unit_locations={\"base\": [[[0, 1, 2, 3]]]},\n",
        "    source_representations = torch.rand(gpt2.config.n_embd)\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a8fd2b8e",
      "metadata": {
        "id": "a8fd2b8e"
      },
      "source": [
        "### Activation Collection with Intervention\n",
        "You can also collect activations with our provided `pv.CollectIntervention` intervention. More importantly, this can be used interchangably with other interventions. You can collect something from an intervened model.\n",
        "\n",
        "**We can basically use this like hooks!**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6e6bd585",
      "metadata": {
        "id": "6e6bd585",
        "outputId": "f323cc9a-b2c2-433d-ef55-6e290011db84"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "loaded model\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import pyvene as pv\n",
        "\n",
        "_, tokenizer, gpt2 = pv.create_gpt2()\n",
        "\n",
        "config = pv.IntervenableConfig({\n",
        "    \"layer\": 10,\n",
        "    \"component\": \"mlp_output\",\n",
        "    \"intervention_type\": pv.CollectIntervention}\n",
        ")\n",
        "\n",
        "pv_gpt2 = pv.IntervenableModel(\n",
        "    config, model=gpt2)\n",
        "\n",
        "collected_activations = pv_gpt2(\n",
        "    base = tokenizer(\n",
        "        \"The capital of Spain is\",\n",
        "        return_tensors=\"pt\"\n",
        "    ), unit_locations={\"sources->base\": 3}\n",
        ")[0][-1]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a9e6e4d9",
      "metadata": {
        "id": "a9e6e4d9"
      },
      "source": [
        "### Intervene on a Single Neuron\n",
        "We want to provide a good user interface so that interventions can be done easily by people with less pytorch or programming experience. Meanwhile, we also want to be flexible and provide the depth of control required for highly specific tasks. Here is an example where we intervene on a specific neuron at a specific head of a layer in a model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d25b6401",
      "metadata": {
        "id": "d25b6401",
        "outputId": "7fbf5386-80a1-4d53-dcf2-5332137b090e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "loaded model\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import pyvene as pv\n",
        "\n",
        "_, tokenizer, gpt2 = pv.create_gpt2()\n",
        "\n",
        "config = pv.IntervenableConfig({\n",
        "    \"layer\": 8,\n",
        "    \"component\": \"head_attention_value_output\",\n",
        "    \"unit\": \"h.pos\",\n",
        "    \"intervention_type\": pv.CollectIntervention}\n",
        ")\n",
        "\n",
        "pv_gpt2 = pv.IntervenableModel(\n",
        "    config, model=gpt2)\n",
        "\n",
        "collected_activations = pv_gpt2(\n",
        "    base = tokenizer(\n",
        "        \"The capital of Spain is\",\n",
        "        return_tensors=\"pt\"\n",
        "    ),\n",
        "    unit_locations={\n",
        "        # GET_LOC is a helper.\n",
        "        # (3,3) means head 3 position 3\n",
        "        \"base\": pv.GET_LOC((3,3))\n",
        "    },\n",
        "    # the notion of subspace is used to target neuron 0.\n",
        "    subspaces=[0]\n",
        ")[0][-1]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "121366c1",
      "metadata": {
        "id": "121366c1"
      },
      "source": [
        "### LMs Generation\n",
        "You can also intervene the generation call of LMs. Here is a simple example where we try to add a vector into the MLP output when the model decodes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f718e2d6",
      "metadata": {
        "id": "f718e2d6",
        "outputId": "df5951e1-f9de-4e83-a910-f6d2fb95a632",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loaded model\n",
            " Happy\n",
            "14628\n",
            "31900\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Once upon a time there was a little girl named Lucy. She was three years old and loved to explore. One day, Lucy was walking in the park when she saw something shiny in the grass. She bent down to pick it up and saw it was a coin. She was so excited and wanted to show it to her mom.\n",
            "\n",
            "But when she tried to pick it up, she realized it was stuck in the ground. She tried to pull it out, but it wouldn't budge\n",
            "\n",
            "Once upon a time there was a little girl named Lucy. She was three years old and loved to explore. One day, Lucy decided to go on an adventure. She put on her shoes and grabbed her hat and set off.\n",
            "\n",
            "As she walked, Lucy noticed a big, dark cave. She was a bit scared but she was also very curious. She decided to go inside. As she walked in, she saw something shiny and sparkly. It was a beautiful necklace! She was so\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import pyvene as pv\n",
        "\n",
        "# built-in helper to get tinystore\n",
        "_, tokenizer, tinystory = pv.create_gpt_neo()\n",
        "emb_happy = tinystory.transformer.wte(\n",
        "    torch.tensor(31900))# 14628))\n",
        "\n",
        "print(tokenizer.decode(14628))\n",
        "print(tokenizer.encode(\" Happy\")[0])\n",
        "print(tokenizer.encode(\" Angry\")[0])\n",
        "\n",
        "pv_tinystory = pv.IntervenableModel([{\n",
        "    \"layer\": l,\n",
        "    \"component\": \"mlp_output\",\n",
        "    \"intervention_type\": pv.AdditionIntervention\n",
        "    } for l in range(tinystory.config.num_layers)],\n",
        "    model=tinystory\n",
        ")\n",
        "# prompt and generate\n",
        "prompt = tokenizer(\n",
        "    \"Once upon a time there was\", return_tensors=\"pt\")\n",
        "unintervened_story, intervened_story = pv_tinystory.generate(\n",
        "    prompt, source_representations=emb_happy*0.1, max_length=100\n",
        ")\n",
        "\n",
        "print(tokenizer.decode(\n",
        "    intervened_story[0],\n",
        "    skip_special_tokens=True\n",
        "))\n",
        "print('')\n",
        "# prompt and generate\n",
        "prompt = tokenizer(\n",
        "    \"Once upon a time there was\", return_tensors=\"pt\")\n",
        "unintervened_story, intervened_story = pv_tinystory.generate(\n",
        "    prompt, source_representations=emb_happy*0.9, max_length=100\n",
        ")\n",
        "print(tokenizer.decode(\n",
        "    intervened_story[0],\n",
        "    skip_special_tokens=True\n",
        "))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Try it yourself\n",
        "\n",
        "We've talked about the paper 'Language Models Implement Simple Word2Vec-style Vector Arithmetic' yesterday.\n",
        "\n",
        "The authors identified that the MLP module of layer 19 of 'gpt2-medium' encodes a **'+_capital_city'** update.\n",
        "\n",
        "For instance, the intermediate outputs on the prompt\n",
        "\n",
        "```\n",
        "prompt_poland =\"\"\"Q: What is the capital of France?\n",
        "A: Paris\n",
        "Q: What is the capital of Poland?\n",
        "A:\"\"\"\n",
        "```\n",
        "\n",
        "looked something like this:\n",
        "```\n",
        "14  St N G P Poland B C Pol A D\n",
        "15  Poland P St Pol Warsaw Polish N B G Germany\n",
        "16  Poland Warsaw Polish Poles Budapest Prague Pol Germany Berlin Moscow\n",
        "17  Poland Warsaw Polish Poles Budapest Prague � Pol Lithuania Moscow\n",
        "18  Poland Warsaw Polish Prague Budapest Poles Moscow � Berlin Kiev\n",
        "19  Warsaw Poland Polish Budapest Prague Moscow Berlin Kiev � Frankfurt\n",
        "20  Warsaw Poland Prague Budapest Polish Moscow Kiev Berlin Frankfurt Brussels\n",
        "21  Warsaw Poland Polish Prague Budapest � Kiev Sz Berlin Moscow\n",
        "22  Warsaw Poland Prague Budapest K W Kiev Sz Moscow Berlin\n",
        "23  Warsaw W K Br Po B L Z P Poland\n",
        "```\n",
        "We were able to show that the MLP update seems to be responsible for the update from Poland -> Warsaw in layer 19."
      ],
      "metadata": {
        "id": "GQ8jYq6yoCYl"
      },
      "id": "GQ8jYq6yoCYl"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### TODO:\n",
        "\n",
        "Use Pyvene to show that MLP layer 19 encodes a '+_capital_city' update for the given prompt.\n",
        "\n",
        "Hint: You want to use prompt_poland as the source, and \"table mug free China table mug free China table mug free\" as the base."
      ],
      "metadata": {
        "id": "4q_V162OqNnG"
      },
      "id": "4q_V162OqNnG"
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import torch\n",
        "import pyvene as pv\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "\n",
        "# 1. Load the model\n",
        "model_name = \"gpt2-medium\"\n",
        "gpt2 = AutoModelForCausalLM.from_pretrained(model_name, attn_implementation=\"eager\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "\n",
        "# TODO: Define base and source prompts\n",
        "prompt_poland =\"\"\"Q: What is the capital of France?\n",
        "A: Paris\n",
        "Q: What is the capital of Poland?\n",
        "A:\"\"\"\n",
        "\n",
        "# prompt_china = \"Only say China: China China China China\"\n",
        "prompt_china = \"table mug free Spain table mug free Spain table mug free\"\n",
        "\n",
        "# TODO: Define a config\n",
        "pv_config = pv.IntervenableConfig(\n",
        "    [{\"layer\": i, \"component\": \"mlp_output\"} for i in range(18, 24)],\n",
        "    intervention_types=pv.VanillaIntervention\n",
        ")\n",
        "\n",
        "# 2. TODO: Wrap the model\n",
        "pv_gpt2 = pv.IntervenableModel(\n",
        "  pv_config, model=gpt2)\n",
        "\n",
        "# Hint: you may need this for your unit_locations\n",
        "# Get the last token position of both models\n",
        "# Tokenize prompts\n",
        "base_tokens = tokenizer(prompt_china, return_tensors=\"pt\")\n",
        "source_tokens = tokenizer(prompt_poland, return_tensors=\"pt\")\n",
        "\n",
        "# Compute last token index for both base and source\n",
        "last_base_idx = base_tokens.input_ids.shape[1] - 1  # Last token index of base\n",
        "last_source_idx = source_tokens.input_ids.shape[1] - 1  # Last token index of source\n",
        "\n",
        "\n",
        "# 3. TODO: Run the intervened model\n",
        "# orig_outputs, intervened_outputs = ...\n",
        "orig_outputs, intervened_outputs = pv_gpt2(\n",
        "  base=tokenizer(prompt_china, return_tensors = \"pt\"),      # Base, i.e., intervened on\n",
        "  sources=tokenizer(prompt_poland, return_tensors = \"pt\"),  # Source, i.e, intervened with\n",
        "  unit_locations = {\"sources->base\": (last_source_idx, last_base_idx)}, # TODO here: I don't want to intervene at token 3, but I want to intervene at the respective last token of source and base (different length!)\n",
        "  output_original_output=True\n",
        ")\n",
        "\n",
        "# Hint: You may want to look at the change in prediction & at the change in probability of a certain capital token ...\n",
        "# Get logits at the last token position\n",
        "orig_logits = orig_outputs.logits[:, last_base_idx, :]  # Shape: (1, vocab_size)\n",
        "intervened_logits = intervened_outputs.logits[:, last_base_idx, :]  # Shape: (1, vocab_size)\n",
        "\n",
        "# Compute probabilities using softmax\n",
        "orig_probs = torch.softmax(orig_logits, dim=-1)\n",
        "intervened_probs = torch.softmax(intervened_logits, dim=-1)\n",
        "\n",
        "# Token ID for \" Beijing\"\n",
        "token_beijing = tokenizer.encode(\" Madrid\")[0]\n",
        "\n",
        "# Extract probability of \"Beijing\" token\n",
        "orig_prob_beijing = orig_probs[0, token_beijing].item()\n",
        "intervened_prob_beijing = intervened_probs[0, token_beijing].item()\n",
        "\n",
        "print(f\"Original probability of 'Beijing': {orig_prob_beijing:.6f}\")\n",
        "print(f\"Intervened probability of 'Beijing': {intervened_prob_beijing:.6f}\")\n",
        "\n",
        "# Get logits\n",
        "orig_logits = orig_outputs.logits\n",
        "intervened_logits = intervened_outputs.logits\n",
        "\n",
        "# Convert logits to token predictions\n",
        "# orig_predictions = orig_logits.argmax(dim=-1)  # Select most likely token at each position\n",
        "orig_predictions = orig_logits[:, -1, :].argmax(dim=-1)  # Only get the final token\n",
        "# intervened_predictions = intervened_logits.argmax(dim=-1)\n",
        "intervened_predictions = intervened_logits[:, -1, :].argmax(dim=-1)  # Only get the final token\n",
        "\n",
        "# Decode token predictions to text\n",
        "orig_text = tokenizer.decode(orig_predictions)\n",
        "intervened_text = tokenizer.decode(intervened_predictions)\n",
        "\n",
        "print(\"Original Output:\", orig_text)\n",
        "print(\"Intervened Output:\", intervened_text)\n"
      ],
      "metadata": {
        "id": "vSA5HDdRMOAW",
        "outputId": "b165741f-56a3-4f95-ef6c-699ba8de77cc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "vSA5HDdRMOAW",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original probability of 'Beijing': 0.000557\n",
            "Intervened probability of 'Beijing': 0.011279\n",
            "Original Output:  Spain\n",
            "Intervened Output:  Spain\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.9"
    },
    "toc-autonumbering": true,
    "toc-showcode": false,
    "toc-showmarkdowntxt": false,
    "toc-showtags": true,
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}